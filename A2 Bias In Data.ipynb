{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: Bias in Data - Ankit Tandon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we're going to be looking at bias in Wikipedia articles. Specifically, we're going to use the ORES API to evaluate the quality of Wikipedia articles from around the world that are about political figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is provided under the MIT license from https://github.com/Ironholds/data-512-a2/blob/master/hcds-a2-bias_demo.ipynb\n",
    "I have modified the headers object and added a return for the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enwiki': {'models': {'wp10': {'version': '0.6.1'}},\n",
       "  'scores': {'757539710': {'wp10': {'score': {'prediction': 'Start',\n",
       "      'probability': {'B': 0.05635270475191951,\n",
       "       'C': 0.17635417131683803,\n",
       "       'FA': 0.001919869734464717,\n",
       "       'GA': 0.005517075264277984,\n",
       "       'Start': 0.732764644204933,\n",
       "       'Stub': 0.027091534727566813}}}},\n",
       "   '783381498': {'wp10': {'score': {'prediction': 'Start',\n",
       "      'probability': {'B': 0.039498449850621085,\n",
       "       'C': 0.06068466061111685,\n",
       "       'FA': 0.0029057427468351755,\n",
       "       'GA': 0.007477221115409147,\n",
       "       'Start': 0.5674464916024892,\n",
       "       'Stub': 0.3219874340735285}}}},\n",
       "   '807355596': {'wp10': {'score': {'prediction': 'Start',\n",
       "      'probability': {'B': 0.04566408685167919,\n",
       "       'C': 0.10144128886317841,\n",
       "       'FA': 0.002651239009002438,\n",
       "       'GA': 0.006433022662730785,\n",
       "       'Start': 0.7675063182740381,\n",
       "       'Stub': 0.07630404433937113}}}}}}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {'User-Agent' : 'https://github.com/ankittandon', 'From' : 'antand@uw.edu'}\n",
    "\n",
    "def get_ores_data(revision_ids, headers):\n",
    "    \n",
    "    # Define the endpoint\n",
    "    endpoint = 'https://ores.wikimedia.org/v3/scores/{project}/?models={model}&revids={revids}'\n",
    "    \n",
    "    # Specify the parameters - smushing all the revision IDs together separated by | marks.\n",
    "    # Yes, 'smush' is a technical term, trust me I'm a scientist.\n",
    "    # What do you mean \"but people trusting scientists regularly goes horribly wrong\" who taught you tha- oh.  \n",
    "    params = {'project' : 'enwiki',\n",
    "              'model'   : 'wp10',\n",
    "              'revids'  : '|'.join(str(x) for x in revision_ids)\n",
    "              }\n",
    "    api_call = requests.get(endpoint.format(**params))\n",
    "    response = api_call.json()\n",
    "    return response\n",
    "#    print(json.dumps(response, indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "# So if we grab some example revision IDs and turn them into a list and then call get_ores_data...\n",
    "example_ids = [783381498, 807355596, 757539710]\n",
    "get_ores_data(example_ids, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is provided under the MIT license from https://github.com/Ironholds/data-512-a2/blob/master/hcds-a2-bias_demo.ipynb\n",
    "I modified this code by adding the encoding = 'utf-8' to ensure the file is read with the proper encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the data from the CSV files\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "revisions = defaultdict(lambda: defaultdict(str))\n",
    "with open('page_data.csv',encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        if row[2] != 'rev_id':\n",
    "            revisions[row[2]]['article_name'] = row[0]\n",
    "            revisions[row[2]]['country'] = row[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid spamming the ORES API, i'll create 100 header chunks that I can send instead of the entire list of headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "chunk = []\n",
    "count = 0\n",
    "for revision in revisions:\n",
    "    chunk.append(revision)\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        chunks.append(chunk)\n",
    "        chunk = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the headers are 'chunked' up, I can append responses from the API to a responses list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for chunk in chunks:\n",
    "    responses.append(get_ores_data(chunk, headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the response object there is a 'prediction' value for each article revision. I'm going to create a dictionary to map the prediction to the revision id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for response in responses:\n",
    "    for r in response['enwiki']['scores']:\n",
    "        if 'score' in response['enwiki']['scores'][r]['wp10']:\n",
    "            revisions[r]['article_quality'] = response['enwiki']['scores'][r]['wp10']['score']['prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the number of predictions we've got to understand the loss in errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47197\n"
     ]
    }
   ],
   "source": [
    "print(len(revisions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's join our dataset with the population data.\n",
    "First we're going to create a dictionary with the geography to population mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = {}\n",
    "with open('WPDS_2018_data.csv',encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        if row[0] != 'Geography':\n",
    "            population[row[0].lower()] = float(row[1].replace(',',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's lookup the countries from our revisions dictionary in the population dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for revision in revisions:\n",
    "    if revisions[revision]['country'].lower() in population:\n",
    "        revisions[revision]['population'] = population[revisions[revision]['country'].lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a simple check to make sure the lookups worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'str'>, {'article_name': \"'Matlelima Hlalele\", 'country': 'Lesotho', 'article_quality': 'Start', 'population': 2.3})\n"
     ]
    }
   ],
   "source": [
    "print(revisions['807355596'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's output this data to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('revisions_data_file.csv', mode='w', encoding='utf-8',newline='') as datafile:\n",
    "    datafilewriter = csv.writer(datafile, delimiter=',', quotechar=\"'\")\n",
    "    datafilewriter.writerow(['country','article_name','revision_id','article_quality','population'])\n",
    "    for revision in revisions:\n",
    "        row = revisions[revision]\n",
    "        datafilewriter.writerow([row['country'],row['article_name'],str(revision),row['article_quality'],row['population']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for analysis. Let's create a few dictionaries for number of articles per country and number of 'good' articles per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_per_country = defaultdict(int)\n",
    "good_articles_per_country = defaultdict(int)\n",
    "for revision in revisions:\n",
    "    articles_per_country[revisions[revision]['country']] += 1\n",
    "    if revisions[revision]['article_quality'] == 'FA' or revisions[revision]['article_quality'] == 'GA':\n",
    "        good_articles_per_country[revisions[revision]['country']] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create dictionaries for number of articles by population of country and percentage of high quality articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_by_population = defaultdict(float)\n",
    "proportion_of_high_quality = defaultdict(float)\n",
    "for country in articles_per_country:\n",
    "    if country in articles_per_country and country.lower() in population:\n",
    "        articles_by_population[country.lower()] = (articles_per_country[country] / population[country.lower()])\n",
    "    if country in good_articles_per_country and country in articles_per_country:\n",
    "        if country.lower() is 'united states':\n",
    "            print(good_articles_per_country[country])\n",
    "        proportion_of_high_quality[country.lower()] = (good_articles_per_country[country] / articles_per_country[country])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_by_articles_by_population = []\n",
    "values_largest_articles_by_population = []\n",
    "smallest_by_articles_by_population = []\n",
    "values_smallest_articles_by_population = []\n",
    "\n",
    "largest_by_good_articles_by_population = []\n",
    "values_largest_good_articles_by_population = []\n",
    "smallest_by_good_articles_by_population = []\n",
    "values_smallest_good_articles_by_population = []\n",
    "\n",
    "for i in range(10):\n",
    "    largest = max(articles_by_population,key=articles_by_population.get)\n",
    "    largest_by_articles_by_population.append(largest)\n",
    "    values_largest_articles_by_population.append(articles_by_population[largest])\n",
    "    del articles_by_population[largest]\n",
    "    smallest = min(articles_by_population,key=articles_by_population.get)\n",
    "    smallest_by_articles_by_population.append(smallest)\n",
    "    values_smallest_articles_by_population.append(articles_by_population[smallest])\n",
    "    del articles_by_population[smallest]\n",
    "    \n",
    "    largest = max(proportion_of_high_quality,key=proportion_of_high_quality.get)\n",
    "    largest_by_good_articles_by_population.append(largest)\n",
    "    values_largest_good_articles_by_population.append(proportion_of_high_quality[largest])\n",
    "    del proportion_of_high_quality[largest]\n",
    "    smallest = min(proportion_of_high_quality,key=proportion_of_high_quality.get)\n",
    "    smallest_by_good_articles_by_population.append(smallest)\n",
    "    values_smallest_good_articles_by_population.append(proportion_of_high_quality[smallest])\n",
    "    del proportion_of_high_quality[smallest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 countries in terms of number of politician articles as a proportion of country population\n",
      "Rank\t\tCountry\t\t\t\tpolitician articles as a proportion of population\n",
      "1\t\ttuvalu\t\t\t\t5500.0\n",
      "2\t\tnauru\t\t\t\t5300.0\n",
      "3\t\tsan marino\t\t\t\t2733.3333333333335\n",
      "4\t\tmonaco\t\t\t\t1000.0\n",
      "5\t\tliechtenstein\t\t\t\t725.0\n",
      "6\t\ttonga\t\t\t\t630.0\n",
      "7\t\tmarshall islands\t\t\t\t616.6666666666667\n",
      "8\t\ticeland\t\t\t\t515.0\n",
      "9\t\tandorra\t\t\t\t425.0\n",
      "10\t\tfederated states of micronesia\t\t\t\t380.0\n",
      "\n",
      "bottom 10 countries in terms of number of politician articles as a proportion of country population\n",
      "Rank\t\tCountry\t\t\t\tpolitician articles as a proportion of population\n",
      "1\t\tindia\t\t\t\t0.7219426821264494\n",
      "2\t\tindonesia\t\t\t\t0.8107088989441931\n",
      "3\t\tchina\t\t\t\t0.8164729516429904\n",
      "4\t\tuzbekistan\t\t\t\t0.8814589665653496\n",
      "5\t\tethiopia\t\t\t\t0.9767441860465116\n",
      "6\t\tzambia\t\t\t\t1.4689265536723164\n",
      "7\t\tkorea, north\t\t\t\t1.5234375\n",
      "8\t\tthailand\t\t\t\t1.6918429003021147\n",
      "9\t\tbangladesh\t\t\t\t1.9471153846153846\n",
      "10\t\tmozambique\t\t\t\t1.9672131147540983\n",
      "\n",
      "top 10 countries in terms of proportion of articles that are predicted as GA or FA a.k.a good articles\n",
      "Rank\t\tCountry\t\t\t\tproportion of good articles\n",
      "1\t\tkorea, north\t\t\t\t0.15384615384615385\n",
      "2\t\trhodesian\t\t\t\t0.13157894736842105\n",
      "3\t\tsaudi arabia\t\t\t\t0.11764705882352941\n",
      "4\t\tcentral african republic\t\t\t\t0.11764705882352941\n",
      "5\t\tromania\t\t\t\t0.11494252873563218\n",
      "6\t\tmauritania\t\t\t\t0.09615384615384616\n",
      "7\t\ttuvalu\t\t\t\t0.09090909090909091\n",
      "8\t\tbhutan\t\t\t\t0.09090909090909091\n",
      "9\t\tdominica\t\t\t\t0.08333333333333333\n",
      "10\t\tunited states\t\t\t\t0.07468123861566485\n",
      "\n",
      "bottom 10 countries in terms of proportion of articles that are predicted as GA or FA a.k.a good articles\n",
      "Rank\t\tCountry\t\t\t\tproportion of good articles\n",
      "1\t\ttanzania\t\t\t\t0.0024509803921568627\n",
      "2\t\tperu\t\t\t\t0.002824858757062147\n",
      "3\t\tczech republic\t\t\t\t0.003937007874015748\n",
      "4\t\tlithuania\t\t\t\t0.004032258064516129\n",
      "5\t\tnigeria\t\t\t\t0.0043859649122807015\n",
      "6\t\tmorocco\t\t\t\t0.004807692307692308\n",
      "7\t\tbolivia\t\t\t\t0.0053475935828877\n",
      "8\t\tbrazil\t\t\t\t0.00539568345323741\n",
      "9\t\tluxembourg\t\t\t\t0.005555555555555556\n",
      "10\t\tsierra leone\t\t\t\t0.006024096385542169\n"
     ]
    }
   ],
   "source": [
    "rank = 1\n",
    "print(\"top 10 countries in terms of number of politician articles as a proportion of country population\")\n",
    "print(\"Rank\\t\\tCountry\\t\\t\\t\\tpolitician articles as a proportion of population\")\n",
    "for i in range(10):\n",
    "    print(str(rank) + \"\\t\\t\" + str(largest_by_articles_by_population[i]) + \"\\t\\t\\t\\t\" + str(values_largest_articles_by_population[i]))\n",
    "    rank += 1\n",
    "print()\n",
    "rank = 1\n",
    "print(\"bottom 10 countries in terms of number of politician articles as a proportion of country population\")\n",
    "print(\"Rank\\t\\tCountry\\t\\t\\t\\tpolitician articles as a proportion of population\")\n",
    "for i in range(10):\n",
    "    print(str(rank) + \"\\t\\t\" + str(smallest_by_articles_by_population[i]) + \"\\t\\t\\t\\t\" + str(values_smallest_articles_by_population[i]))\n",
    "    rank += 1\n",
    "print()\n",
    "rank = 1\n",
    "print(\"top 10 countries in terms of proportion of articles that are predicted as GA or FA a.k.a good articles\")\n",
    "print(\"Rank\\t\\tCountry\\t\\t\\t\\tproportion of good articles\")\n",
    "for i in range(10):\n",
    "    print(str(rank) + \"\\t\\t\" + str(largest_by_good_articles_by_population[i]) + \"\\t\\t\\t\\t\" + str(values_largest_good_articles_by_population[i]))\n",
    "    rank += 1\n",
    "print()   \n",
    "rank = 1\n",
    "print(\"bottom 10 countries in terms of proportion of articles that are predicted as GA or FA a.k.a good articles\")\n",
    "print(\"Rank\\t\\tCountry\\t\\t\\t\\tproportion of good articles\")\n",
    "for i in range(10):\n",
    "    print(str(rank) + \"\\t\\t\" + str(smallest_by_good_articles_by_population[i]) + \"\\t\\t\\t\\t\" + str(values_smallest_good_articles_by_population[i]))\n",
    "    rank += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
